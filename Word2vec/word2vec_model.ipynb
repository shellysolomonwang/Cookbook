{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"word2vec_model.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"ze8HFrt7tmfu","colab_type":"code","colab":{},"outputId":"34e512ef-eb54-4bcf-a940-7a1850bb73e4"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","import pandas as pd\n","import nltk,string\n","import os\n","import glob\n","import multiprocessing\n","import gensim\n","import numpy as np\n","import nltk\n","import re\n","from random import shuffle\n","\n","# download the punkt tokenizer\n","nltk.download('punkt')\n","print(\"The punkt tokenizer is downloaded\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The punkt tokenizer is downloaded\n"],"name":"stdout"},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/shelly/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"r0B74CRktmf2","colab_type":"code","colab":{}},"source":["# load review data\n","file_path = '/Users/shelly/Google Drive/BIA660/final/Web_Scraping/New_20191128/New/*.csv'\n","addrs = glob.glob(file_path)\n","# load data from Newly scraped results\n","for idx, addr in enumerate(addrs):\n","    if idx == 0:\n","        review_df = pd.read_csv(addr)\n","    else:\n","        new_df = pd.read_csv(addr)\n","        review_df = pd.concat([review_df, new_df], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0sXNeNODtmf6","colab_type":"code","colab":{},"outputId":"c21e5939-83fa-4d7f-87e6-3f8b681b100f"},"source":["review_df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5243, 7)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"r3oZMNkitmf-","colab_type":"code","colab":{},"outputId":"d75346c2-6026-4f97-80a4-aef7d043ebe1"},"source":["# load data from previous clusters\n","dir_list = ['/Users/shelly/Google Drive/BIA660/final/Reviews/Cluster0',\n","            '/Users/shelly/Google Drive/BIA660/final/Reviews/Cluster1-1',\n","            '/Users/shelly/Google Drive/BIA660/final/Reviews/Cluster2']\n","\n","for folder in dir_list:\n","    addrs = glob.glob(folder+'/*.csv')\n","    for addr in addrs:\n","        new_df = pd.read_csv(addr).head(50)\n","        review_df = pd.concat([review_df, new_df], ignore_index=True)\n","del review_df['Unnamed: 0']"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/Users/shelly/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n","of pandas will change to not sort by default.\n","\n","To accept the future behavior, pass 'sort=False'.\n","\n","To retain the current behavior and silence the warning, pass 'sort=True'.\n","\n","  # Remove the CWD from sys.path while we load stuff.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"6W8Ie_PetmgD","colab_type":"code","colab":{},"outputId":"e37691a8-c046-47e3-c80e-6340e30f7d8c"},"source":["review_df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6143, 7)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"P1SsuOr4tmgG","colab_type":"code","colab":{}},"source":["# load cluster results\n","cluster_df = pd.read_excel('/Users/shelly/Google Drive/BIA660/final/Cluster_Result.xlsx')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NkaIK_H4tmgJ","colab_type":"code","colab":{}},"source":["df = pd.merge(review_df, cluster_df, left_on='book_title', right_on='title')\n","# replace nan in the review field with ''\n","df['review'] = df['review'].replace(np.nan, '', regex=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XYVGz5zPtmgN","colab_type":"code","colab":{}},"source":["# save it as a csv\n","df.to_csv('review_join_cluster_6000.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QSvki_dWtmgP","colab_type":"code","colab":{}},"source":["# df.to_csv('review_join_cluster.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z3RU_WdntmgT","colab_type":"code","colab":{},"outputId":"1ceb55e3-4a6d-47b7-fb80-91b4396b2c32"},"source":["df.groupby(['cluster']).count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>book_title</th>\n","      <th>format</th>\n","      <th>helpful</th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","      <th>star_x</th>\n","      <th>title_x</th>\n","      <th>index</th>\n","      <th>No.</th>\n","      <th>book_link</th>\n","      <th>...</th>\n","      <th>star_y</th>\n","      <th>review_amount</th>\n","      <th>kindle_price</th>\n","      <th>paperback_price</th>\n","      <th>hardcover_price</th>\n","      <th>intro</th>\n","      <th>book_rank</th>\n","      <th>product_desc</th>\n","      <th>abt_author</th>\n","      <th>key_words</th>\n","    </tr>\n","    <tr>\n","      <th>cluster</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>1615</td>\n","      <td>1536</td>\n","      <td>1219</td>\n","      <td>1615</td>\n","      <td>1615</td>\n","      <td>1615</td>\n","      <td>1615</td>\n","      <td>1615</td>\n","      <td>1615</td>\n","      <td>1615</td>\n","      <td>...</td>\n","      <td>1615</td>\n","      <td>1615</td>\n","      <td>1453</td>\n","      <td>1235</td>\n","      <td>442</td>\n","      <td>1585</td>\n","      <td>1615</td>\n","      <td>1019</td>\n","      <td>548</td>\n","      <td>1615</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>2140</td>\n","      <td>2114</td>\n","      <td>1694</td>\n","      <td>2140</td>\n","      <td>2140</td>\n","      <td>2124</td>\n","      <td>2124</td>\n","      <td>2140</td>\n","      <td>2140</td>\n","      <td>2140</td>\n","      <td>...</td>\n","      <td>2140</td>\n","      <td>2140</td>\n","      <td>2080</td>\n","      <td>1840</td>\n","      <td>1397</td>\n","      <td>2140</td>\n","      <td>1919</td>\n","      <td>1359</td>\n","      <td>441</td>\n","      <td>2140</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2298</td>\n","      <td>2274</td>\n","      <td>1542</td>\n","      <td>2298</td>\n","      <td>2298</td>\n","      <td>2298</td>\n","      <td>2297</td>\n","      <td>2298</td>\n","      <td>2298</td>\n","      <td>2298</td>\n","      <td>...</td>\n","      <td>2298</td>\n","      <td>2298</td>\n","      <td>2088</td>\n","      <td>1415</td>\n","      <td>1682</td>\n","      <td>2298</td>\n","      <td>1785</td>\n","      <td>1735</td>\n","      <td>147</td>\n","      <td>2298</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows Ã— 22 columns</p>\n","</div>"],"text/plain":["         book_title  format  helpful  review  sentiment  star_x  title_x  \\\n","cluster                                                                    \n","0              1615    1536     1219    1615       1615    1615     1615   \n","1              2140    2114     1694    2140       2140    2124     2124   \n","2              2298    2274     1542    2298       2298    2298     2297   \n","\n","         index   No.  book_link  ...  star_y  review_amount  kindle_price  \\\n","cluster                          ...                                        \n","0         1615  1615       1615  ...    1615           1615          1453   \n","1         2140  2140       2140  ...    2140           2140          2080   \n","2         2298  2298       2298  ...    2298           2298          2088   \n","\n","         paperback_price  hardcover_price  intro  book_rank  product_desc  \\\n","cluster                                                                     \n","0                   1235              442   1585       1615          1019   \n","1                   1840             1397   2140       1919          1359   \n","2                   1415             1682   2298       1785          1735   \n","\n","         abt_author  key_words  \n","cluster                         \n","0               548       1615  \n","1               441       2140  \n","2               147       2298  \n","\n","[3 rows x 22 columns]"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"JdA6mbZ3tmgV","colab_type":"code","colab":{},"outputId":"8c3ebb68-3628-47cd-cc2c-fd619a724fd9"},"source":["# import natural language toolkit\n","import nltk\n","# download the punkt tokenizer\n","nltk.download('punkt')\n","print(\"The punkt tokenizer is downloaded\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/shelly/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":31},{"output_type":"stream","text":["The punkt tokenizer is downloaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"426uYVUItmgY","colab_type":"code","colab":{},"outputId":"8c9b2a3b-5776-43e9-ce51-d6c987767836"},"source":["list(df[df['cluster']==1]['review'])[1241]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Received a 20 year old book. Not the book pictured'"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"s9VCphNwtmgb","colab_type":"code","colab":{},"outputId":"37ca0e5e-11b1-42cb-e0b1-2192cc5bcf7b"},"source":["cluster = 1\n","raw_corpus = u''.join(df[df['cluster']==1]['review']+\" \")\n","print(\"Raw Corpus contains {0:,} characters\".format(len(raw_corpus)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Raw Corpus contains 1,627,166 characters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sroZchlptmgd","colab_type":"code","colab":{},"outputId":"a91fed17-a48f-4b86-e092-645880ee882b"},"source":["# Load the punkt tokenizer\n","tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n","print(\"The punkt tokenizer is loaded\")\n","# we tokenize the raw string into raw sentences\n","raw_sentences = tokenizer.tokenize(raw_corpus)\n","print(\"We have {0:,} raw sentences\".format(len(raw_sentences)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The punkt tokenizer is loaded\n","We have 18,017 raw sentences\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CRphamWqtmgg","colab_type":"code","colab":{}},"source":["# Clean and split sentence into words\n","def clean_and_split_str(string):\n","    strip_special_chars = re.compile(\"[^A-Za-z]+\")\n","    string = re.sub(strip_special_chars, \" \", string)\n","    return string.strip().split()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j3SVeGBCtmgk","colab_type":"code","colab":{},"outputId":"0e03c0bf-2943-4b36-87f8-434a3963cfe3"},"source":["# clean each raw sentences and build the list of sentences\n","sentences = []\n","for raw_sent in raw_sentences:\n","    if len(raw_sent) > 0:\n","        sentences.append(clean_and_split_str(raw_sent))\n","print(\"We have {0:,} clean sentences\".format(len(sentences)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["We have 18,017 clean sentences\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kyFCwF_utmgm","colab_type":"code","colab":{},"outputId":"c8d59815-3ba6-4917-8e0d-32f877373e1b"},"source":["print(raw_sentences[10])\n","print()\n","print(sentences[10])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["While the compliments are wonderful on my new appearance and it's exciting to be 2 sizes smaller, the real changes (I hoped) were happening on the inside.\n","\n","['While', 'the', 'compliments', 'are', 'wonderful', 'on', 'my', 'new', 'appearance', 'and', 'it', 's', 'exciting', 'to', 'be', 'sizes', 'smaller', 'the', 'real', 'changes', 'I', 'hoped', 'were', 'happening', 'on', 'the', 'inside']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C5PxOyCltmgp","colab_type":"code","colab":{},"outputId":"02f7bbf3-1897-4ab9-a9d6-ec14c0cbb601"},"source":["token_count = sum([len(sentence) for sentence in sentences])\n","print(\"The dataset corpus contains {0:,} tokens\".format(token_count))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The dataset corpus contains 300,598 tokens\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZTUINOzjtmgu","colab_type":"code","colab":{}},"source":["word2vec_model = gensim.models.word2vec.Word2Vec(\n","    sg=1,\n","    seed=1,\n","    #Seed for the RNG, to make the result reproducible\n","    workers=multiprocessing.cpu_count(),\n","    #Number of threads to run in parallel\n","    size=300,\n","    #Dimensionality of the resulting word vectors\n","    min_count=3,\n","    #Minimum word count threshold\n","    window=7)\n","    #Context window length\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4myzcB9stmgy","colab_type":"code","colab":{},"outputId":"23bc169a-52b9-4c9e-e735-73b7731b736b"},"source":["word2vec_model.build_vocab(sentences=sentences)\n","print(\"The vocabulary is built\")\n","# print(\"Word2Vec vocabulary length: \", len(list(word2vec_model.vocabulary)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The vocabulary is built\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u2UeF1j0tmg1","colab_type":"code","colab":{},"outputId":"0ae57887-fdae-477f-f564-88067fd25f5f"},"source":["\n","#Start training the model\n","for epoch in range(30):\n","    # shuffle the documents in each epoch\n","    shuffle(sentences)\n","word2vec_model.train(sentences=sentences,\n","                     total_examples=word2vec_model.corpus_count,\n","                     epochs=1)\n","print(\"Training finished\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(215889, 300598)"]},"metadata":{"tags":[]},"execution_count":44},{"output_type":"stream","text":["Training finished\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Bo3yKey9tmg3","colab_type":"code","colab":{},"outputId":"398c18ae-eb36-48e7-b66d-850201f9408f"},"source":["#Save the model\n","word2vec_model.save(\"result2/word2vec_model_trained_on_cluster0.w2v\")\n","print(\"Model saved\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5CSifkcntmg6","colab_type":"code","colab":{}},"source":["def my_word_2_vec(cluster):\n","    # Convert all the review text into a long string and print its length\n","    raw_corpus = u''.join(df[df['cluster']==cluster]['review']+\" \")\n","    print(\"Raw Corpus contains {0:,} characters\".format(len(raw_corpus)))\n","    # Load the punkt tokenizer\n","    tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n","    print(\"The punkt tokenizer is loaded\")\n","    # we tokenize the raw string into raw sentences\n","    raw_sentences = tokenizer.tokenize(raw_corpus)\n","    print(\"We have {0:,} raw sentences\".format(len(raw_sentences)))\n","    \n","    # Clean and split sentence into words\n","    def clean_and_split_str(string):\n","        strip_special_chars = re.compile(\"[^A-Za-z]+\")\n","        string = re.sub(strip_special_chars, \" \", string)\n","        return string.strip().split()\n","    # clean each raw sentences and build the list of sentences\n","    sentences = []\n","    for raw_sent in raw_sentences:\n","        if len(raw_sent) > 0:\n","            sentences.append(clean_and_split_str(raw_sent))\n","    print(\"We have {0:,} clean sentences\".format(len(sentences)))\n","    \n","    token_count = sum([len(sentence) for sentence in sentences])\n","    print(\"The dataset corpus contains {0:,} tokens\".format(token_count))\n","    \n","    word2vec_model = gensim.models.word2vec.Word2Vec(\n","                                                    sg=1,\n","                                                    seed=1,\n","                                                    #Seed for the RNG, to make the result reproducible\n","                                                    workers=multiprocessing.cpu_count(),\n","                                                    #Number of threads to run in parallel\n","                                                    size=300,\n","                                                    #Dimensionality of the resulting word vectors\n","                                                    min_count=3,\n","                                                    #Minimum word count threshold\n","                                                    window=7)\n","                                                    #Context window length\n","    word2vec_model.build_vocab(sentences=sentences)\n","    print(\"The vocabulary is built\")\n","    for epoch in range(30):\n","    # shuffle the documents in each epoch\n","        shuffle(sentences)\n","        word2vec_model.train(sentences=sentences,\n","                             total_examples=word2vec_model.corpus_count,\n","                             epochs=1)\n","    print(\"Training finished\")\n","    word2vec_model.save(\"./results/word2vec_model_trained_on_cluster\"+str(cluster)+\".w2v\")\n","    print(\"Model saved\")\n","    return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HkJrVVn0tmg8","colab_type":"code","colab":{},"outputId":"4c31d53c-5e4d-44c6-c0fa-31dfe2b458eb"},"source":["my_word_2_vec(0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Raw Corpus contains 965,846 characters\n","The punkt tokenizer is loaded\n","We have 11,229 raw sentences\n","We have 11,229 clean sentences\n","The dataset corpus contains 179,511 tokens\n","The vocabulary is built\n","Training finished\n","Model saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yRjwiEHYtmg_","colab_type":"code","colab":{},"outputId":"e0685b7c-beee-4706-be34-8ad8e4a255af"},"source":["my_word_2_vec(1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Raw Corpus contains 1,627,166 characters\n","The punkt tokenizer is loaded\n","We have 18,017 raw sentences\n","We have 18,017 clean sentences\n","The dataset corpus contains 300,598 tokens\n","The vocabulary is built\n","Training finished\n","Model saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WTBJkWg2tmhB","colab_type":"code","colab":{},"outputId":"60f4aaa3-9f02-482d-926b-b78684307129"},"source":["my_word_2_vec(2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Raw Corpus contains 1,465,886 characters\n","The punkt tokenizer is loaded\n","We have 16,172 raw sentences\n","We have 16,172 clean sentences\n","The dataset corpus contains 270,775 tokens\n","The vocabulary is built\n","Training finished\n","Model saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BU2K6SpttmhE","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}